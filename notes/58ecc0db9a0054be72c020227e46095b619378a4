10   Exercises

    1. su Search the web for "spoof newspaper headlines", to find such
       gems as: British Left Waffles on Falkland Islands, and Juvenile
       Court to Try Shooting Defendant. Manually tag these headlines to
       see if knowledge of the part-of-speech tags removes the ambiguity.
    2. su Working with someone else, take turns to pick a word that can be
       either a noun or a verb (e.g. contest); the opponent has to predict
       which one is likely to be the most frequent in the Brown corpus;
       check the opponent's prediction, and tally the score over several
       turns.
    3. su Tokenize and tag the following sentence: They wind back the
       clock, while we chase after the wind. What different pronunciations
       and parts of speech are involved?
    4. su Review the mappings in
       [https://www.nltk.org/book/ch05.html#tab-linguistic-objects]3.1.
       Discuss any other examples of mappings you can think of. What type
       of information do they map from and to?
    5. su Using the Python interpreter in interactive mode, experiment
       with the dictionary examples in this chapter. Create a dictionary
       d, and add some entries. What happens if you try to access a
       non-existent entry, e.g. d['xyz']?
    6. su Try deleting an element from a dictionary d, using the syntax
       del d['abc']. Check that the item was deleted.
    7. su Create two dictionaries, d1 and d2, and add some entries to
       each. Now issue the command d1.update(d2). What did this do? What
       might it be useful for?
    8. su Create a dictionary e, to represent a single lexical entry for
       some word of your choice. Define keys like headword,
       part-of-speech, sense, and example, and assign them suitable
       values.
    9. su Satisfy yourself that there are restrictions on the distribution
       of go and went, in the sense that they cannot be freely
       interchanged in the kinds of contexts illustrated in
       [https://www.nltk.org/book/ch05.html#ex-go](3d) in
       [https://www.nltk.org/book/ch05.html#sec-how-to-determine-the-categ
       ory-of-a-word]7.
   10. su Train a unigram tagger and run it on some new text. Observe that
       some words are not assigned a tag. Why not?
   11. su Learn about the affix tagger (type help(nltk.AffixTagger)).
       Train an affix tagger and run it on some new text. Experiment with
       different settings for the affix length and the minimum word
       length. Discuss your findings.
   12. su Train a bigram tagger with no backoff tagger, and run it on some
       of the training data. Next, run it on some new data. What happens
       to the performance of the tagger? Why?
   13. su We can use a dictionary to specify the values to be substituted
       into a formatting string. Read Python's library documentation for
       formatting strings http://docs.python.org/lib/typesseq-strings.html
       and use this method to display today's date in two different
       formats.
   14. 0R Use sorted() and set() to get a sorted list of tags used in the
       Brown corpus, removing duplicates.
   15. 0R Write programs to process the Brown Corpus and find answers to
       the following questions:
         1. Which nouns are more common in their plural form, rather than
            their singular form? (Only consider regular plurals, formed
            with the -s suffix.)
         2. Which word has the greatest number of distinct tags. What are
            they, and what do they represent?
         3. List tags in order of decreasing frequency. What do the 20
            most frequent tags represent?
         4. Which tags are nouns most commonly found after? What do these
            tags represent?
   16. 0R Explore the following issues that arise in connection with the
       lookup tagger:
         1. What happens to the tagger performance for the various model
            sizes when a backoff tagger is omitted?
         2. Consider the curve in
            [https://www.nltk.org/book/ch05.html#fig-tag-lookup]4.2;
            suggest a good size for a lookup tagger that balances memory
            and performance. Can you come up with scenarios where it would
            be preferable to minimize memory usage, or to maximize
            performance with no regard for memory usage?
   17. 0R What is the upper limit of performance for a lookup tagger,
       assuming no limit to the size of its table? (Hint: write a program
       to work out what percentage of tokens of a word are assigned the
       most likely tag for that word, on average.)
   18. 0R Generate some statistics for tagged data to answer the following
       questions:
         1. What proportion of word types are always assigned the same
            part-of-speech tag?
         2. How many words are ambiguous, in the sense that they appear
            with at least two tags?
         3. What percentage of word tokens in the Brown Corpus involve
            these ambiguous words?
   19. 0R The evaluate() method works out how accurately the tagger
       performs on this text. For example, if the supplied tagged text was
       [('the', 'DT'), ('dog', 'NN')] and the tagger produced the output
       [('the', 'NN'), ('dog', 'NN')], then the score would be 0.5. Let's
       try to figure out how the evaluation method works:
         1. A tagger t takes a list of words as input, and produces a list
            of tagged words as output. However, t.evaluate() is given
            correctly tagged text as its only parameter. What must it do
            with this input before performing the tagging?
         2. Once the tagger has created newly tagged text, how might the
            evaluate() method go about comparing it with the original
            tagged text and computing the accuracy score?
         3. Now examine the source code to see how the method is
            implemented. Inspect nltk.tag.api.__file__ to discover the
            location of the source code, and open this file using an
            editor (be sure to use the api.py file and not the compiled
            api.pyc binary file).
   20. 0R Write code to search the Brown Corpus for particular words and
       phrases according to tags, to answer the following questions:
         1. Produce an alphabetically sorted list of the distinct words
            tagged as MD.
         2. Identify words that can be plural nouns or third person
            singular verbs (e.g. deals, flies).
         3. Identify three-word prepositional phrases of the form IN + DET
            + NN (eg. in the lab).
         4. What is the ratio of masculine to feminine pronouns?
   21. 0R In [https://www.nltk.org/book/ch03.html#tab-absolutely]3.1 we
       saw a table involving frequency counts for the verbs adore, love,
       like, prefer and preceding qualifiers absolutely and definitely.
       Investigate the full range of adverbs that appear before these four
       verbs.
   22. 0R We defined the regexp_tagger that can be used as a fall-back
       tagger for unknown words. This tagger only checks for cardinal
       numbers. By testing for particular prefix or suffix strings, it
       should be possible to guess other tags. For example, we could tag
       any word that ends with -s as a plural noun. Define a regular
       expression tagger (using RegexpTagger()) that tests for at least
       five other patterns in the spelling of words. (Use inline
       documentation to explain the rules.)
   23. 0R Consider the regular expression tagger developed in the
       exercises in the previous section. Evaluate the tagger using its
       accuracy() method, and try to come up with ways to improve its
       performance. Discuss your findings. How does objective evaluation
       help in the development process?
   24. 0R How serious is the sparse data problem? Investigate the
       performance of n-gram taggers as n increases from 1 to 6. Tabulate
       the accuracy score. Estimate the training data required for these
       taggers, assuming a vocabulary size of 10^5 and a tagset size of
       10^2.
   25. 0R Obtain some tagged data for another language, and train and
       evaluate a variety of taggers on it. If the language is
       morphologically complex, or if there are any orthographic clues
       (e.g. capitalization) to word classes, consider developing a
       regular expression tagger for it (ordered after the unigram tagger,
       and before the default tagger). How does the accuracy of your
       tagger(s) compare with the same taggers run on English data?
       Discuss any issues you encounter in applying these methods to the
       language.
   26. 0R [https://www.nltk.org/book/ch05.html#code-baseline-tagger]4.1
       plotted a curve showing change in the performance of a lookup
       tagger as the model size was increased. Plot the performance curve
       for a unigram tagger, as the amount of training data is varied.
   27. 0R Inspect the confusion matrix for the bigram tagger t2 defined in
       [https://www.nltk.org/book/ch05.html#sec-n-gram-tagging]5, and
       identify one or more sets of tags to collapse. Define a dictionary
       to do the mapping, and evaluate the tagger on the simplified data.
   28. 0R Experiment with taggers using the simplified tagset (or make one
       of your own by discarding all but the first character of each tag
       name). Such a tagger has fewer distinctions to make, but much less
       information on which to base its work. Discuss your findings.
   29. 0R Recall the example of a bigram tagger which encountered a word
       it hadn't seen during training, and tagged the rest of the sentence
       as None. It is possible for a bigram tagger to fail part way
       through a sentence even if it contains no unseen words (even if the
       sentence was used during training). In what circumstance can this
       happen? Can you write a program to find some examples of this?
   30. 0R Preprocess the Brown News data by replacing low frequency words
       with UNK, but leaving the tags untouched. Now train and evaluate a
       bigram tagger on this data. How much does this help? What is the
       contribution of the unigram tagger and default tagger now?
   31. 0R Modify the program in
       [https://www.nltk.org/book/ch05.html#code-baseline-tagger]4.1 to
       use a logarithmic scale on the x-axis, by replacing pylab.plot()
       with pylab.semilogx(). What do you notice about the shape of the
       resulting plot? Does the gradient tell you anything?
   32. 0R Consult the documentation for the Brill tagger demo function,
       using help(nltk.tag.brill.demo). Experiment with the tagger by
       setting different values for the parameters. Is there any trade-off
       between training time (corpus size) and performance?
   33. 0R Write code that builds a dictionary of dictionaries of sets. Use
       it to store the set of POS tags that can follow a given word having
       a given POS tag, i.e. word[i] -> tag[i] -> tag[i+1].
   34. * There are 264 distinct words in the Brown Corpus having exactly
       three possible tags.
         1. Print a table with the integers 1..10 in one column, and the
            number of distinct words in the corpus having 1..10 distinct
            tags in the other column.
         2. For the word with the greatest number of distinct tags, print
            out sentences from the corpus containing the word, one for
            each possible tag.
   35. * Write a program to classify contexts involving the word must
       according to the tag of the following word. Can this be used to
       discriminate between the epistemic and deontic uses of must?
   36. * Create a regular expression tagger and various unigram and n-gram
       taggers, incorporating backoff, and train them on part of the Brown
       corpus.
         1. Create three different combinations of the taggers. Test the
            accuracy of each combined tagger. Which combination works
            best?
         2. Try varying the size of the training corpus. How does it
            affect your results?
   37. * Our approach for tagging an unknown word has been to consider the
       letters of the word (using RegexpTagger()), or to ignore the word
       altogether and tag it as a noun (using nltk.DefaultTagger()). These
       methods will not do well for texts having new words that are not
       nouns. Consider the sentence I like to blog on Kim's blog. If blog
       is a new word, then looking at the previous tag (TO versus NP$)
       would probably be helpful. I.e. we need a default tagger that is
       sensitive to the preceding tag.
         1. Create a new kind of unigram tagger that looks at the tag of
            the previous word, and ignores the current word. (The best way
            to do this is to modify the source code for UnigramTagger(),
            which presumes knowledge of object-oriented programming in
            Python.)
         2. Add this tagger to the sequence of backoff taggers (including
            ordinary trigram and bigram taggers that look at words), right
            before the usual default tagger.
         3. Evaluate the contribution of this new unigram tagger.
   38. * Consider the code in
       [https://www.nltk.org/book/ch05.html#sec-n-gram-tagging]5 which
       determines the upper bound for accuracy of a trigram tagger. Review
       Abney's discussion concerning the impossibility of exact tagging
       [https://www.nltk.org/book/bibliography.html#abney1996pst](Church,
       Young, & Bloothooft, 1996). Explain why correct tagging of these
       examples requires access to other kinds of information than just
       words and tags. How might you estimate the scale of this problem?
   39. * Use some of the estimation techniques in nltk.probability, such
       as Lidstone or Laplace estimation, to develop a statistical tagger
       that does a better job than n-gram backoff taggers in cases where
       contexts encountered during testing were not seen during training.
   40. * Inspect the diagnostic files created by the Brill tagger
       rules.out and errors.out. Obtain the demonstration code by
       accessing the source code (at http://www.nltk.org/code) and create
       your own version of the Brill tagger. Delete some of the rule
       templates, based on what you learned from inspecting rules.out. Add
       some new rule templates which employ contexts that might help to
       correct the errors you saw in errors.out.
   41. * Develop an n-gram backoff tagger that permits "anti-n-grams" such
       as ["the", "the"] to be specified when a tagger is initialized. An
       anti-ngram is assigned a count of zero and is used to prevent
       backoff for this n-gram (e.g. to avoid estimating P(the | the) as
       just P(the)).
   42. * Investigate three different ways to define the split between
       training and testing data when developing a tagger using the Brown
       Corpus: genre (category), source (fileid), and sentence. Compare
       their relative performance and discuss which method is the most
       legitimate. (You might use n-fold cross validation, discussed in
       [https://www.nltk.org/book/ch06.html#sec-evaluation]3, to improve
       the accuracy of the evaluations.)
   43. * Develop your own NgramTagger class that inherits from NLTK's
       class, and which encapsulates the method of collapsing the
       vocabulary of the tagged training and testing data that was
       described in this chapter. Make sure that the unigram and default
       backoff taggers have access to the full vocabulary.

   About this document...

   UPDATED FOR NLTK 3.0. This is a chapter from Natural Language
   Processing with Python, by [http://stevenbird.net/]Steven Bird,
   [http://homepages.inf.ed.ac.uk/ewan/]Ewan Klein and
   [http://ed.loper.org/]Edward Loper, Copyright © 2019 the authors. It is
   distributed with the Natural Language Toolkit [http://nltk.org/],
   Version 3.0, under the terms of the Creative Commons
   Attribution-Noncommercial-No Derivative Works 3.0 United States License
   [[http://creativecommons.org/licenses/by-nc-nd/3.0/us/]http://creativec
   ommons.org/licenses/by-nc-nd/3.0/us/].

   This document was built on Wed 4 Sep 2019 11:40:48 ACST
